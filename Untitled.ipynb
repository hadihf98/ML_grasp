{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from utils.loader import Loader\n",
    "from utils.image import draw_around_box, draw_pose, get_area_of_interest\n",
    "from utils.trainer import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 22002 grasp attempts.\n",
      "Loaded action of episode 2020-06-02-15-02-02-585 with reward: 1.0 at pose: {'x': -0.002845421342589834, 'y': 0.003349629672480432, 'z': 0.011470451871526954, 'a': -0.09805190767050087, 'b': -0.00023496112321907958, 'c': -9.82719293047829e-05, 'd': 0.07}\n",
      "{'pose': {'x': -0.002845421342589834, 'y': 0.003349629672480432, 'z': 0.011470451871526954, 'a': -0.09805190767050087, 'b': -0.00023496112321907958, 'c': -9.82719293047829e-05, 'd': 0.07}, 'type': 'grasp', 'safe': 1, 'reward': 1.0, 'collision': False, 'estimated_reward': 0.9999464750289917, 'method': 'Top5', 'images': {'rd-v': {'info': {'pixel_size': 2000.0, 'min_depth': 0.2199999988079071, 'max_depth': 0.4099999964237213}, 'pose': {'x': -0.00011724935688089966, 'y': -2.5390135137193237e-05, 'z': 0.34997342528090897, 'a': 1.1827800472135883e-06, 'b': -0.00022421208875591028, 'c': -0.00012080165907850046}}, 'rc-v': {'info': {'pixel_size': 2000.0, 'min_depth': 0.2199999988079071, 'max_depth': 0.4099999964237213}, 'pose': {'x': -0.00011724935688089966, 'y': -2.5390135137193237e-05, 'z': 0.34997342528090897, 'a': 1.1827800472135883e-06, 'b': -0.00022421208875591028, 'c': -0.00012080165907850046}}}, 'bin': 'Left', 'bin_episode': '2020-06-02-15-02-02-585', 'final_pose': {'x': 0.47658009433402, 'y': -0.12798555089985822, 'z': 0.021515846763221796, 'a': 1.4728134680354676, 'b': -0.00048731245093225687, 'c': 0.00021982333795048068, 'd': 0.03553641521930695}, 'execution_time': 19.05026388168335, 'box_data': {'contour': [[0.08499999999999999, 0.13449999999999998, 0.068], [0.08499999999999999, -0.1475, 0.068], [-0.089, -0.1475, 0.068], [-0.089, 0.13449999999999998, 0.068]], 'color': [117, 66, 0]}, 'episode_id': '2020-06-02-15-02-02-585'}\n",
      "1\n",
      "2\n",
      "3\n",
      "Episode keys:  ['id', 'actions']\n",
      "Action keys:  ['pose', 'type', 'safe', 'reward', 'collision', 'estimated_reward', 'method', 'images', 'bin', 'bin_episode', 'final_pose', 'execution_time', 'box_data', 'episode_id']\n",
      "Training set length: 15419\n",
      "Validation set length: 3223\n",
      "Test set length: 3360\n"
     ]
    }
   ],
   "source": [
    "# 1. Init loader, this will load the (non-image) dataset into memory\n",
    "loader = Loader()\n",
    "print(f'Dataset has {len(loader)} grasp attempts.')\n",
    "\n",
    "\n",
    "# 2. Load an action and/or image\n",
    "episode_index = 0\n",
    "action = loader.get_action(episode_index, action_id=0)\n",
    "print(f\"Loaded action of episode {action['episode_id']} with reward: {action['reward']} at pose: {action['pose']}\")\n",
    "print(action)\n",
    "\n",
    "rgbd_image = loader.get_image(episode_index, action_id=0, camera='rcd')\n",
    "\n",
    "# You can also load action and images at once\n",
    "action, rgb_image = loader.get_action(episode_index, action_id=0, images=['rc'])\n",
    "cv2.imshow('image', rgb_image.mat)  # OpenCV uses uint8 format\n",
    "cv2.waitKey(0)\n",
    "print(\"1\")\n",
    "\n",
    "# 3. Draw action and box on image for visualization\n",
    "draw_around_box(rgbd_image, action['box_data'])\n",
    "draw_pose(rgbd_image, action['pose'])\n",
    "cv2.imshow('rgb image', rgbd_image.mat[:, :, :3])\n",
    "cv2.imshow('depth image', rgbd_image.mat[:, :, 3])\n",
    "cv2.waitKey(0)\n",
    "print(\"2\")\n",
    "\n",
    "# 4. Get image area by an affine transformation. By specifying size_result, we can scale the final image down.\n",
    "area = get_area_of_interest(rgbd_image, action['pose'], size_cropped=(200, 200), size_result=(100, 100))  # [px]\n",
    "cv2.imshow('area', area)\n",
    "cv2.waitKey(0)\n",
    "print(\"3\")\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 5. Iterate over all actions\n",
    "for index, episode in enumerate(loader.yield_episodes()):\n",
    "    print('Episode keys: ', list(episode.keys()))\n",
    "    print('Action keys: ', list(episode['actions'][0].keys()))\n",
    "\n",
    "    # loader.get_image(index, action_id=0, camera='rcd')\n",
    "    break\n",
    "\n",
    "\n",
    "# 6. Split into Training / Validation / Test set\n",
    "training_set, validation_set, test_set = Trainer.split(loader.episodes, seed=42)\n",
    "print(f'Training set length: {len(training_set)}')\n",
    "print(f'Validation set length: {len(validation_set)}')\n",
    "print(f'Test set length: {len(test_set)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15419/15419 [00:38<00:00, 400.78it/s]\n"
     ]
    }
   ],
   "source": [
    "training_data_img = []\n",
    "y = []\n",
    "IMG_SIZE_y = 100\n",
    "IMG_SIZE_x = 156\n",
    "\n",
    "def create_training_data():\n",
    "    for i in tqdm(range(len(training_set))):\n",
    "        episode_index = i\n",
    "        action = loader.get_action(episode_index, action_id=0)\n",
    "        path = '/home/hadi/Documents/ML_thesis/grasp-learning-dataset-v1/grasp-learning-dataset/images/'+action['episode_id']+'/0-rd-v.png' \n",
    "        training_img_array = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        new_training_array = cv2.resize(training_img_array, (IMG_SIZE_x, IMG_SIZE_y))\n",
    "#         new_training_array =new_training_array/255.0\n",
    "#         labels = action['reward']\n",
    "#         y.append(labels)\n",
    "        training_data_img.append([new_training_array, action['reward']])\n",
    "\n",
    "create_training_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for sample in training_data_img[:3]:\n",
    "    print(sample[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, label in training_data_img:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE_x, IMG_SIZE_y, 1)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "338/338 [==============================] - 31s 92ms/step - loss: 0.3886 - accuracy: 0.8757 - val_loss: 0.7028 - val_accuracy: 0.7748\n",
      "Epoch 2/3\n",
      "338/338 [==============================] - 19s 58ms/step - loss: 0.3832 - accuracy: 0.8775 - val_loss: 0.7042 - val_accuracy: 0.7748\n",
      "Epoch 3/3\n",
      "338/338 [==============================] - 19s 56ms/step - loss: 0.3808 - accuracy: 0.8775 - val_loss: 0.5440 - val_accuracy: 0.7748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb5ef109fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, LeakyReLU, BatchNormalization\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "X = X/255.0\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3,3),input_shape = X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy', optimizer ='adam', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X, y , batch_size = 32, epochs = 3,\n",
    " validation_split = 0.3)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(64, (5,5),strides = (1,1) ,kernel_regularizer = tf.keras.regularizers.l1_l2(l1=0.0, l2=0.3), bias_regularizer = tf.keras.regularizers.l1_l2(l1=0.0, l2=0.3), input_shape = X.shape[1:]))\n",
    "# model.add(LeakyReLU(alpha = 0.1))\n",
    "# model.add(BatchNormalization(trainable = False))\n",
    "\n",
    "# model.add(Conv2D(64, (5,5),dilation_rate=(1,1),strides = (1,1) ,kernel_regularizer = tf.keras.regularizers.l1_l2(l1=0.0, l2=0.3), bias_regularizer = tf.keras.regularizers.l1_l2(l1=0.0, l2=0.3), input_shape = X.shape[1:]))\n",
    "# model.add(LeakyReLU(alpha = 0.1))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv2D(64, (5,5),dilation_rate=(1,1),strides = (1,1) ,kernel_regularizer = tf.keras.regularizers.l1_l2(l1=0.0, l2=0.3), bias_regularizer = tf.keras.regularizers.l1_l2(l1=0.0, l2=0.3), input_shape = X.shape[1:]))\n",
    "# model.add(LeakyReLU(alpha = 0.1))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.4))\n",
    "\n",
    "# model.add(Conv2D(64, (6,6),dilation_rate=(1,1),strides = (1,1) ,kernel_regularizer = tf.keras.regularizers.l1_l2(l1=0.0, l2=0.3), bias_regularizer = tf.keras.regularizers.l1_l2(l1=0.0, l2=0.3), input_shape = X.shape[1:]))\n",
    "# model.add(LeakyReLU(alpha = 0.1))\n",
    "# # model.add(BatchNormalization(training = False))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(Conv2D(64, (1,1),dilation_rate=(1,1),strides = (1,1) ,kernel_regularizer = tf.keras.regularizers.l1_l2(l1=0.0, l2=0.3), bias_regularizer = tf.keras.regularizers.l1_l2(l1=0.0, l2=0.3), input_shape = X.shape[1:]))\n",
    "# model.add(LeakyReLU(alpha = 0.1))\n",
    "# # model.add(BatchNormalization(training = False))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(Conv2D(3,(1,1),dilation_rate=(1,1),bias_regularizer = tf.keras.regularizers.l2(0.2) ))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15419, 156, 100, 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
